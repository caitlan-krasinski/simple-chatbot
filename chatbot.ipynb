{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import re\n",
    "import random\n",
    "\n",
    "import spacy\n",
    "space = spacy.load('en_core_web_sm')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers import Input,Embedding,Bidirectional,LSTM,Dense,Concatenate\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Intent.json') as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^a-zA-z]', ' ', text)\n",
    "    text = re.sub(r'[ ]+', ' ', text)\n",
    "    text = text.strip()\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "targets = []\n",
    "classes = []\n",
    "intent_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    if intent['intent'] not in classes:\n",
    "        classes.append(intent['intent'])\n",
    "        \n",
    "    if intent['intent'] not in intent_dict:\n",
    "        intent_dict[intent['intent']] = []\n",
    "        \n",
    "    for text in intent['text']:\n",
    "        inputs.append(clean_text(text))\n",
    "        targets.append(intent['intent'])\n",
    "        \n",
    "    for response in intent['responses']:\n",
    "        intent_dict[intent['intent']].append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', oov_token='*UNKNOWN*')\n",
    "tokenizer.fit_on_texts(inputs)\n",
    "seq = tokenizer.texts_to_sequences(inputs)\n",
    "seq = tf.keras.preprocessing.sequence.pad_sequences(seq, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dict = {}\n",
    "cat_target = []\n",
    "i = -1\n",
    "for target in targets:\n",
    "    if target not in target_dict.values():\n",
    "        target_dict[i+1] = target\n",
    "        i+=1   \n",
    "    cat_target.append(i)\n",
    "    \n",
    "cat_tensor = tf.keras.utils.to_categorical(cat_target, num_classes=len(target_dict), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = seq\n",
    "y = cat_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 512)         60928     \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 256)               656384    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 22)                2838      \n",
      "=================================================================\n",
      "Total params: 753,046\n",
      "Trainable params: 753,046\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 512))\n",
    "model.add(Bidirectional(tf.keras.layers.LSTM(128, dropout=0.5)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer = 'adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "29/29 [==============================] - 3s 14ms/step - loss: 3.0856 - accuracy: 0.1094\n",
      "Epoch 2/100\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 2.9736 - accuracy: 0.2073\n",
      "Epoch 3/100\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 2.5331 - accuracy: 0.3533\n",
      "Epoch 4/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 1.6944 - accuracy: 0.4844\n",
      "Epoch 5/100\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 1.0457 - accuracy: 0.6434\n",
      "Epoch 6/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.5304 - accuracy: 0.8239\n",
      "Epoch 7/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.4535 - accuracy: 0.8353\n",
      "Epoch 8/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.3224 - accuracy: 0.9343\n",
      "Epoch 9/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.3259 - accuracy: 0.8968\n",
      "Epoch 10/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.1247 - accuracy: 0.9958\n",
      "Epoch 11/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0748 - accuracy: 0.9760\n",
      "Epoch 12/100\n",
      "29/29 [==============================] - 0s 13ms/step - loss: 0.0329 - accuracy: 1.0000\n",
      "Epoch 13/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0195 - accuracy: 1.0000\n",
      "Epoch 14/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0071 - accuracy: 1.0000\n",
      "Epoch 15/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 16/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 17/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 18/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0038 - accuracy: 1.0000\n",
      "Epoch 19/100\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0047 - accuracy: 1.0000\n",
      "Epoch 20/100\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 21/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 22/100\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 23/100\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 24/100\n",
      "29/29 [==============================] - 0s 15ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 25/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 26/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 27/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 28/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 29/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 30/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 31/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 32/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 33/100\n",
      "29/29 [==============================] - 0s 14ms/step - loss: 0.0148 - accuracy: 0.9940\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14a0b4340>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "model.fit(x, y, epochs=100, batch_size=5, verbose=1, callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def module(text):\n",
    "    seq = []\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    text = space(repr(text))\n",
    "    \n",
    "    for word in text:\n",
    "        if word.text in tokenizer.word_index:\n",
    "            seq.append(tokenizer.word_index[word.text])\n",
    "        else:\n",
    "            seq.append(tokenizer.word_index['*UNKNOWN*'])\n",
    "            \n",
    "    seq = tf.expand_dims(seq, 0)\n",
    "    pred = model(seq)\n",
    "    pred_class = np.argmax(pred.numpy(), axis=1)\n",
    "    \n",
    "    return random.choice(intent_dict[target_dict[pred_class[0]]]), target_dict[pred_class[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'quit' to close the chat\n",
      "You: Hello\n",
      "ChatBot: Hi human, please tell me your GeniSys user \n",
      "\n",
      "You: How are you?\n",
      "ChatBot: Hi, how are you? I am great thanks! Please tell me your GeniSys user \n",
      "\n",
      "You: I am good, thank you!\n",
      "ChatBot: OK! Hola <HUMAN>, how can I help you? \n",
      "\n",
      "You: what is your name?\n",
      "ChatBot: My real name is GeniSys \n",
      "\n",
      "You: are you alive?\n",
      "ChatBot: Thanks, I was trained that way \n",
      "\n",
      "You: quit\n"
     ]
    }
   ],
   "source": [
    "print(\"Type 'quit' to close the chat\")\n",
    "while True:\n",
    "    text = input('You: ')\n",
    "    if text.lower() == 'quit':\n",
    "        break\n",
    "    response, class_pred = module(text)\n",
    "    print('ChatBot: {} \\n'.format(response))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
